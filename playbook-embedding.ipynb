{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embedding NIM LangChain Playbook\n",
    "\n",
    "In LLM and retrieval-augmented generation (RAG) workflows, embeddings transform text into vectors that capture semantic meaning. This enables efficient search for contextually relevant documents based on a user's query. These documents are then provided as additional context to the LLM, enhancing its ability to generate accurate responses. \n",
    "\n",
    "This playbook goes over how to use the NeMo Retriever Text Embedding NIM (Text Embedding NIM) with LangChain for a RAG workflow using the `NVIDIAEmbeddings` class. First, it shows how to generate embeddings from a user query. Then, it uses this approach to embed a document, store the embeddings in a vector store, and finally uses the embeddings in a LangChain Expression Language (LCEL) chain to help the LLM answer a question about the NVIDIA H200.\n",
    "\n",
    "## Use NVIDIA NIM for LLMs \n",
    "\n",
    "First, initialize the LLM for this playbook. This playbook uses NVIDIA NIM for LLMs. You can access the chat models using the `ChatNVIDIA` class from the `langchain-nvidia-ai-endpoints` package, which contains LangChain integrations for building applications with models on  NVIDIA NIM for large language models (LLMs). For more information, see the [ChatNVIDIA](https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/) documentation.\n",
    "\n",
    "Once the Llama3-8b-instruct NIM has been deployed on your infrastructure, you can access it using the `ChatNVIDIA` class, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to a LLM NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the LLM is ready, you can use it with LangChain's `ChatPromptTemplate`, which is a class for structuring multi-turn conversations and formatting inputs for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Say you don't know if you don't have this information.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with the LLM in the LangChain Expression Language (LCEL) chain, use the `invoke` method, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A GPU (Graphics Processing Unit) is a specialized computer chip that handles graphics and compute tasks, providing faster rendering and increased performance for gaming and graphics-intensive applications. In contrast, a CPU (Central Processing Unit) is the \"brain\" of the computer, handling general-purpose computing tasks such as instructions, calculations, and memory management.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"What's the difference between a GPU and a CPU?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure what the \"A\" in the NVIDIA A100 stands for. It's named after Andy Gredley, a lawyer at Stanford University who had a circle in his name that resembles an \"A\", with \"100\" referring to the 1000 millimeters in diameter form factor.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"What does the A in the NVIDIA A100 stand for?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, ask a question about the NVIDIA H200 GPU. Since the knowledge cutoff for many LLMs is late 2022 or early 2023, the model might not have access to any information after that timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have specific information on the NVIDIA H200's memory specifications. Can you please provide more context or details about the NVIDIA H200 you're referring to?\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"How much memory does the NVIDIA H200 have?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings with Text Embedding NIM\n",
    "\n",
    "To answer the previous question, build a simple [retrieval-augmented generation (RAG) pipeline](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/).\n",
    "\n",
    "The following example demonstrates how to use LangChain to interact with Text Embedding NIM using the `NVIDIAEmbeddings` Python class from the same `langchain-nvidia-ai-endpoints` package as the first example. Be sure that Text Embedding NIM is running. Since this example uses the `nvidia/nv-embedqa-e5-v5` Text Embeddimg NIM, update `model` accordingly if you are using a different Text Embedding NIM.\n",
    "\n",
    "Generate embeddings from a user query with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0251007080078125,\n",
       " -0.038055419921875,\n",
       " 0.035980224609375,\n",
       " -0.061309814453125,\n",
       " 0.056396484375,\n",
       " -0.001224517822265625,\n",
       " 0.01220703125,\n",
       " -0.04010009765625,\n",
       " -0.0258941650390625,\n",
       " -0.029815673828125]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "# Initialize and connect to a NeMo Retriever Text Embedding NIM (nvidia/nv-embedqa-e5-v5) running at localhost:8000\n",
    "embedding_model = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "                                   base_url=\"http://localhost:8001/v1\")\n",
    "\n",
    "# Create vector embeddings of the query\n",
    "embedding_model.embed_query(\"How much memory does the NVIDIA H200 have?\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load a PDF of the [NVIDIA H200 Datasheet](https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446). This document becomes the knowledge base that the LLM uses to retrieve relevant information to answer questions.\n",
    "\n",
    "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) that load various types of documents (HTML, PDF, code) from many different sources and locations (private s3 buckets, public websites). This example uses the LangChain [`PyPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html) to load the datasheet about the NVIDIA H200 Tensor Core GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf', 'page': 0}, page_content='NVIDIA H200 Tensor Core GPU\\u2002|\\u2002Datasheet\\u2002|\\u20021\\nNVIDIA H200 Tensor Core GPU\\nSupercharging AI and HPC workloads.\\nHigher Performance With Larger, Faster Memory\\nThe NVIDIA H200 Tensor Core GPU supercharges generative AI and high-\\nperformance computing (HPC) workloads with game-changing performance \\nand memory capabilities. \\nBased on the NVIDIA Hopper™ architecture, the NVIDIA H200 is the first GPU to \\noffer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—\\nthat’s nearly double the capacity of the NVIDIA H100 Tensor Core GPU with \\n1.4X more memory bandwidth. The H200’s larger and faster memory accelerates \\ngenerative AI and large language models, while advancing scientific computing for \\nHPC workloads with better energy efficiency and lower total cost of ownership. \\nUnlock Insights With High-Performance LLM Inference\\nIn the ever-evolving landscape of AI, businesses rely on large language models to \\naddress a diverse range of inference needs. An AI inference accelerator must deliver the \\nhighest throughput at the lowest TCO when deployed at scale for a massive user base. \\nThe H200 doubles inference performance compared to H100 GPUs when handling \\nlarge language models such as Llama2 70B.\\n.\\nPreliminary specifications. May be subject to change.\\nLlama2 13B: ISL 128, OSL 2K | Throughput | H100 SXM 1x GPU BS 64 | H200 SXM 1x GPU BS 128\\nGPT-3 175B: ISL 80, OSL 200 | x8 H100 SXM GPUs BS 64 | x8 H200 SXM GPUs BS 128\\nLlama2 70B: ISL 2K, OSL 128 | Throughput | H100 SXM 1x GPU BS 8 | H200 SXM 1x GPU BS 32.\\nKey Features\\n > 141GB of HBM3e GPU memory\\n > 4.8TB/s of memory bandwidth\\n > 4 petaFLOPS of FP8 performance\\n > 2X LLM inference performance\\n > 110X HPC performance\\nDatasheet')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf\")\n",
    "\n",
    "document = loader.load()\n",
    "document[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once documents have been loaded, they are often transformed. One method of transformation is known as **chunking**, which breaks down large pieces of text, such as the text from a long document, into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/). \n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/), such as text splitters. In this example, we use a [``RecursiveCharacterTextSplitter``](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html). The ``RecursiveCharacterTextSplitter`` is divides a large body of text into smaller chunks based on a specified chunk size. It employs recursion as its core mechanism for splitting text, utilizing a predefined set of characters, such as \"\\n\\n\", \"\\n\", \" \", and \"\", to determine where splits should occur. The process begins by attempting to split the text using the first character in the set. If the resulting chunks are still larger than the desired chunk size, it proceeds to the next character in the set and attempts to split again. This process continues until all chunks adhere to the specified maximum chunk size.\n",
    "\n",
    "There are some nuanced complexities to text splitting since semantically related text, in theory, should be kept together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks from the document: 17\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "print(\"Number of chunks from the document:\", len(document_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet demonstrates how to create vector embeddings for a single document. This step is not necessary for the RAG pipeline, but included here for demonstrative purposes. The example uses the embedding model to convert the text chunks into a vectors. It displays only the first 10 elements of this vector from the first document chunk to get a glimpse of what these embeddings look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04156494140625,\n",
       " -0.035552978515625,\n",
       " 0.06524658203125,\n",
       " -0.050384521484375,\n",
       " 0.0848388671875,\n",
       " -0.02410888671875,\n",
       " 0.0245208740234375,\n",
       " -0.02685546875,\n",
       " -0.0136566162109375,\n",
       " -0.003902435302734375]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract text (page content) from the document chunks\n",
    "page_contents = [doc.page_content for doc in document_chunks]\n",
    "\n",
    "# Create vector embeddings from the document\n",
    "embedding_model.embed_documents(page_contents)[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Document Embeddings in the Vector Store\n",
    "\n",
    "Once the document embeddings are generated, they are stored in a vector store. When a user query is received, you can:\n",
    "\n",
    "<ol>\n",
    "<li>Embed the query</li>\n",
    "<li>Perform a similarity search in the vector store to retrieve the most relevant document embeddings</li>\n",
    "<li>Use the retrieved documents to generate a response to the user's query</li>\n",
    "</ol>\n",
    "\n",
    "A vector store takes care of storing the embedded data and performing a vector search. LangChain provides support for a [variety of vector stores](https://python.langchain.com/docs/integrations/vectorstores/), we'll be using FAISS for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(document_chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Text Embedding NIM with LCEL\n",
    "\n",
    "The next example integrates the vector database with the LLM. A [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/modules/chains/) combines these components together. It then formulates the prompt placeholders (context and question) and pipes them to our LLM connector to answer the original question from the first example (`How much memory does the NVIDIA H200 have?`) with embeddings from the `NVIDIA H200 datasheet` document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Do not hallucinate. Say you don't know if you don't have this information.\"\n",
    "        # \"Answer the question using only the context\"\n",
    "        \"\\n\\nQuestion: {question}\\n\\nContext: {context}\"\n",
    "    ),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever(),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NVIDIA H200 has 141 gigabytes (GB) of HBM3e memory.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"How much memory does the NVIDIA H200 have?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document does not explicitly state what the 'H' in NVIDIA H200 stands for.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What does the 'H' in the NVIDIA H200 stand for?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
