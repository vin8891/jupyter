{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Reranking NIM LangChain Playbook\n",
    "\n",
    "Reranking is crucial for achieving high accuracy and efficiency in retrieval pipelines. It plays a vital role, particularly when the pipeline incorporates citations from diverse datastores, where each datastore may employ its own unique similarity scoring algorithm. Reranking serves two primary purposes:\n",
    "\n",
    "<ol>\n",
    "    <li>Improving accuracy for individual citations within each datastore.</li>\n",
    "    <li>Integrating results from multiple datastores to provide a cohesive and relevant set of citations.</li>\n",
    "</ol>\n",
    "\n",
    "This playbook goes over how to use the NeMo Retriever Text Reranking NIM (Text Reranking NIM) with LangChain for document compression and retrieval via the `NVIDIARerank` class.\n",
    "\n",
    "## Use NVIDIA NIM for LLMs \n",
    "\n",
    "First, initialize the LLM for this playbook. This playbook uses NVIDIA NIM for LLMs. You can access the chat models using the `ChatNVIDIA` class from the `langchain-nvidia-ai-endpoints` package, which contains LangChain integrations for building applications with models on  NVIDIA NIM for large language models (LLMs). For more information, see the [ChatNVIDIA](https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/) documentation.\n",
    "\n",
    "Once the Llama3-8b-instruct NIM has been deployed on your infrastructure, you can access it using the `ChatNVIDIA` class, as shown in the following example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to a LLM NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the LLM is ready, use LangChain's `ChatPromptTemplate` class to structure multi-turn conversations and format inputs for the language model, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Say you don't know if you don't have this information.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with the LLM in the LangChain Expression Language (LCEL) chain, use the `invoke` method, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A GPU (Graphics Processing Unit) is a specialized computer chip that handles graphics and compute tasks, providing faster rendering and increased performance for gaming and graphics-intensive applications. In contrast, a CPU (Central Processing Unit) is the \"brain\" of the computer, handling general-purpose computing tasks such as instructions, calculations, and memory management.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"What's the difference between a GPU and a CPU?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next ask the following question about the NVIDIA H200 GPU. Since the knowledge cutoff for many LLMs is late 2022 or early 2023, the model might not have access to information after that timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"H\" in NVIDIA H200 stands for HGX H100, which is a data center GPU solution, with \"H\" in HGX H100 indicating it is designed for High-Performance Computing.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"What does the H in the NVIDIA H200 stand for?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  I'm sorry, at the moment I don't have information on what the 'H' in the NVIDIA H200 stands for. It could possibly be a model-specific identifier or code. You might want to check NVIDIA's official documentation or contact them directly for clarification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking with Text Reranking NIM\n",
    "\n",
    "To answer the previous question, build a simple retrieval and reranking pipeline to find the most relevant piece of information to the query.\n",
    "\n",
    "Load the [NVIDIA H200 Datasheet](https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446) to use in the retrieval pipeline. LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) for various types of documents, such as HTML, PDF, and code, from sources and locations such as private S3 buckets and public websites. The following example uses a LangChain [`PyPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html) to load a datasheet about the NVIDIA H200 Tensor Core GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf', 'page': 0}, page_content='NVIDIA H200 Tensor Core GPU\\u2002|\\u2002Datasheet\\u2002|\\u20021\\nNVIDIA H200 Tensor Core GPU\\nSupercharging AI and HPC workloads.\\nHigher Performance With Larger, Faster Memory\\nThe NVIDIA H200 Tensor Core GPU supercharges generative AI and high-\\nperformance computing (HPC) workloads with game-changing performance \\nand memory capabilities. \\nBased on the NVIDIA Hopper™ architecture, the NVIDIA H200 is the first GPU to \\noffer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—\\nthat’s nearly double the capacity of the NVIDIA H100 Tensor Core GPU with \\n1.4X more memory bandwidth. The H200’s larger and faster memory accelerates \\ngenerative AI and large language models, while advancing scientific computing for \\nHPC workloads with better energy efficiency and lower total cost of ownership. \\nUnlock Insights With High-Performance LLM Inference\\nIn the ever-evolving landscape of AI, businesses rely on large language models to \\naddress a diverse range of inference needs. An AI inference accelerator must deliver the \\nhighest throughput at the lowest TCO when deployed at scale for a massive user base. \\nThe H200 doubles inference performance compared to H100 GPUs when handling \\nlarge language models such as Llama2 70B.\\n.\\nPreliminary specifications. May be subject to change.\\nLlama2 13B: ISL 128, OSL 2K | Throughput | H100 SXM 1x GPU BS 64 | H200 SXM 1x GPU BS 128\\nGPT-3 175B: ISL 80, OSL 200 | x8 H100 SXM GPUs BS 64 | x8 H200 SXM GPUs BS 128\\nLlama2 70B: ISL 2K, OSL 128 | Throughput | H100 SXM 1x GPU BS 8 | H200 SXM 1x GPU BS 32.\\nKey Features\\n > 141GB of HBM3e GPU memory\\n > 4.8TB/s of memory bandwidth\\n > 4 petaFLOPS of FP8 performance\\n > 2X LLM inference performance\\n > 110X HPC performance\\nDatasheet')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf\")\n",
    "\n",
    "document = loader.load()\n",
    "document[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once documents have been loaded, they are often transformed. One method of transformation is known as **chunking**, which breaks down large pieces of text, such as a long document, into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/).\n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/), such as text splitters. The following example uses a [``RecursiveCharacterTextSplitter``](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html). The ``RecursiveCharacterTextSplitter`` is divides a large body of text into smaller chunks based on a specified chunk size. It employs recursion as its core mechanism for splitting text, utilizing a predefined set of characters, such as \"\\n\\n\", \"\\n\", \" \", and \"\", to determine where splits should occur. The process begins by attempting to split the text using the first character in the set. If the resulting chunks are still larger than the desired chunk size, it proceeds to the next character in the set and attempts to split again. This process continues until all chunks adhere to the specified maximum chunk size.\n",
    "\n",
    "There are some nuanced complexities to text splitting since, in theory, semantically related text should be kept together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks from the document: 17\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "print(\"Number of chunks from the document:\", len(document_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how to use LangChain to interact with Text Reranking NIM using the `NVIDIAReranking` class from the same `langchain-nvidia-ai-endpoints` package as the first example. Be sure that you have the NeMo Retriever Text Reranking NIM running before this step. `nvidia/nv-rerankqa-mistral-4b-v3` is used in the following example, update `model` accordingly if you use a different Text Reranking NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "\n",
    "query = \"What does the H in the NVIDIA H200 stand for?\"\n",
    "\n",
    "# Initialize and connect to a NeMo Retriever Text Reranking NIM running at localhost:8000\n",
    "reranker = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "                        base_url=\"http://localhost:8002/v1\")\n",
    "\n",
    "reranked_chunks = reranker.compress_documents(query=query,\n",
    "                                              documents=document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section shows the results of using Text Reranking NIM to rerank the document chunks based on a relevance score from the query to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 16.3125, Page Content: NVIDIA H200 Tensor Core GPU | Datasheet | 1\n",
      "NVIDIA H200 Tensor Core GPU\n",
      "Supercharging AI and HPC workloads.\n",
      "Higher Performance With Larger, Faster Memory\n",
      "The NVIDIA H200 Tensor Core GPU supercharges generative AI and high-\n",
      "performance computing (HPC) workloads with game-changing performance \n",
      "and memory capabilities. \n",
      "Based on the NVIDIA Hopper™ architecture, the NVIDIA H200 is the first GPU to \n",
      "offer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score: 10.1953125, Page Content: NVIDIA H200 Tensor Core GPU | Datasheet | 3\n",
      "AI Acceleration for Mainstream Enterprise Servers With  \n",
      "H200 NVL\n",
      "NVIDIA H200 NVL is ideal for lower-power, air-cooled enterprise rack designs that \n",
      "require flexible configurations, delivering acceleration for every AI and HPC workload \n",
      "regardless of size. With up to four GPUs connected by NVIDIA NVLink™ and a 1.5X \n",
      "memory increase, large language model (LLM) inference can be accelerated up to...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score: 7.47265625, Page Content: NVIDIA H200 Tensor Core GPU | Datasheet | 2\n",
      "Supercharge High-Performance Computing\n",
      "Memory bandwidth is crucial for HPC applications, as it enables faster data \n",
      "transfer and reduces complex processing bottlenecks. For memory-intensive \n",
      "HPC applications like simulations, scientific research, and artificial intelligence, \n",
      "the H200’s higher memory bandwidth ensures that data can be accessed and \n",
      "manipulated efficiently, leading to 110X faster time to results....\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score: 6.1171875, Page Content: memory increase, large language model (LLM) inference can be accelerated up to \n",
      "1.7X and HPC applications achieve up to 1.3X more performance over the H100 NVL.\n",
      "Enterprise-Ready: AI Software Streamlines Development \n",
      "and Deployment \n",
      "NVIDIA H200 NVL comes with a five-year NVIDIA AI Enterprise subscription and \n",
      "simplifies the way you build an enterprise AI-ready platform. H200 accelerates \n",
      "AI development and deployment for production-ready generative AI solutions,...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score: 5.09765625, Page Content: Server Options NVIDIA HGX™ H200 partner and NVIDIA-\n",
      "Certified Systems™ with 4 or 8 GPUs\n",
      "NVIDIA MGX™ H200 NVL partner and \n",
      "NVIDIA-Certified Systems with up to 8 GPUs\n",
      "NVIDIA AI Enterprise Add-on Included\n",
      "1. Preliminary specifications. May be subject to change. \n",
      "2. With sparsity.\n",
      "Ready to Get Started?\n",
      "To learn more about the NVIDIA H200 Tensor Core GPU, \n",
      "visit nvidia.com/h200\n",
      "© 2024 NVIDIA Corporation and affiliates. All rights reserved. NVIDIA, the NVIDIA logo, HGX, Hopper, MGX, NIM,...\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for chunks in reranked_chunks:\n",
    "\n",
    "    # Access the metadata of the document\n",
    "    metadata = chunks.metadata\n",
    "\n",
    "    # Get the page content\n",
    "    page_content = chunks.page_content\n",
    "    \n",
    "    # Print the relevance score if it exists in the metadata, followed by page content\n",
    "    if 'relevance_score' in metadata:\n",
    "        print(f\"Relevance Score: {metadata['relevance_score']}, Page Content: {page_content}...\")\n",
    "    print(f\"{'-' * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Text Reranking NIM with LCEL\n",
    "\n",
    "One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "[Contextual compression](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/contextual_compression/) is a technique to improve retrieval systems by:\n",
    "<ol>\n",
    "    <li> Addressing the challenge of handling unknown future queries when ingesting data.</li>\n",
    "    <li> Reducing irrelevant text in retrieved documents to improve LLM response quality and efficiency.</li>\n",
    "    <li> Compressing individual documents and filtering out irrelevant ones based on the query context.</li>\n",
    "</ol>\n",
    "\n",
    "The Contextual Compression Retriever requires:\n",
    "\n",
    "* A base retriever\n",
    "* A document Compressor\n",
    "\n",
    "It works by:\n",
    "<ol>\n",
    "    <li> Passing queries to the base retriever</li>\n",
    "    <li> Sending retrieved documents through the Document Compressor</li>\n",
    "    <li> Shortening the list of documents by reducing content or removing irrelevant documents entirely</li>\n",
    "</ol>\n",
    "\n",
    "The following example demonstrates how to use the Text Reranking NIM as a document compressor with LangChain.\n",
    "\n",
    "First, initialize an embedding model to embed the query and document chunks. This example uses the Text Embedding NIM that is already deployed at the beginning of the LaunchPad lab. You can access this model using the `NVIDIAEmbeddings` class, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedding_model = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "                                   base_url=\"http://localhost:8001/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll initialize a simple vector store retriever and store the document chunks of the `NVIDIA H200 datasheet`. LangChain provides support for a [great selection of vector stores](https://python.langchain.com/docs/integrations/vectorstores/), we'll be using FAISS for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "retriever = FAISS.from_documents(document_chunks, embedding=embedding_model).as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the base retriever with a `ContextualCompressionRetriever` class, using `NVRerank` as a document compressor, as shown in the following example. As previously mentioned, `nv-rerankqa-mistral-4b-v3` is used for this step, be sure to update `model` accordingly if a different Text Reranking NIM is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "\n",
    "# Re-initialize and connect to a NeMo Retriever Text Reranking NIM running at localhost:8000\n",
    "compressor = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "                          base_url=\"http://localhost:8002/v1\")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, ask the LLM the same question about the \"H\" in NVIDIA H200 again but with the retrieval and reranking pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does the H in the NVIDIA H200 stand for?',\n",
       " 'result': 'I don\\'t know. The text doesn\\'t explicitly mention what \"H\" stands for.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = \"What does the H in the NVIDIA H200 stand for?\"\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
